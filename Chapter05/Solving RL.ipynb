{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodTruck:\n",
    "    def __init__(self):\n",
    "        self.v_demand = [100, 200, 300, 400]\n",
    "        self.p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = self.v_demand[-1]\n",
    "        self.days = ['Mon', 'Tue', 'Wed', \n",
    "                     'Thu', 'Fri', \"Weekend\"]\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                            + [(d, i) for d in self.days[1:] \n",
    "                                for i in [0, 100, 200, 300]]\n",
    "    \n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = {}\n",
    "        result['next_day'] = self.days[self.days.index(day) \\\n",
    "                                       + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, \n",
    "                                           inventory \n",
    "                                           + action)\n",
    "        result['cost'] = self.unit_cost * action \n",
    "        result['sales'] = min(result['starting_inventory'], \n",
    "                              demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] \\\n",
    "            = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "    \n",
    "    def get_transition_prob(self, state, action):\n",
    "        next_s_r_prob = {}\n",
    "        for ix, demand in enumerate(self.v_demand):\n",
    "            result = self.get_next_state_reward(state, \n",
    "                                                action, \n",
    "                                                demand)\n",
    "            next_s = (result['next_day'],\n",
    "                      result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            prob = self.p_demand[ix]\n",
    "            if (next_s, reward) not in next_s_r_prob:\n",
    "                next_s_r_prob[next_s, reward] = prob\n",
    "            else:\n",
    "                next_s_r_prob[next_s, reward] += prob\n",
    "        return next_s_r_prob\n",
    "    \n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        state = (self.day, self.inventory)\n",
    "        return state\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        return day == \"Weekend\"\n",
    "    \n",
    "    def step(self, action):\n",
    "        demand = random.choices(self.v_demand, self.p_demand)[0]\n",
    "        result = self.get_next_state_reward((self.day, \n",
    "                                             self.inventory), \n",
    "                                       action, \n",
    "                                       demand)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "        return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating an arbitrary policy\n",
    "foodtruck = FoodTruck()\n",
    "rewards = []\n",
    "for i_episode in range(10000):\n",
    "    state = foodtruck.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        day, inventory = state\n",
    "        action = max(0, 300 - inventory)\n",
    "        state, reward, done, info = foodtruck.step(action) \n",
    "        ep_reward += reward\n",
    "    rewards.append(ep_reward)\n",
    "statistics.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single day expected reward\n",
    "ucost = 4\n",
    "uprice = 7\n",
    "v_demand = [100, 200, 300, 400]\n",
    "p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "inv = 400\n",
    "profit = uprice * sum(p *min(v, inv) for p, v in zip(v_demand, p_demand)) - inv * ucost\n",
    "print(profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {} \n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "        policy[s] = prob_a\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_update(env, v, s, prob_a, gamma):\n",
    "    expected_value = 0\n",
    "    for a in prob_a:\n",
    "        prob_next_s_r = env.get_transition_prob(s, a)\n",
    "        for next_s, r in prob_next_s_r:\n",
    "            expected_value += prob_a[a] \\\n",
    "                            * prob_next_s_r[next_s, r] \\\n",
    "                            * (r + gamma * v[next_s])\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iter=100, \n",
    "                      v = None, eps=0.1, gamma=1):\n",
    "    if not v:\n",
    "        v = {s: 0 for s in env.state_space}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in v:\n",
    "            if not env.is_terminal(s):\n",
    "                v_old = v[s]\n",
    "                prob_a = policy[s]\n",
    "                v[s] = expected_update(env, v, \n",
    "                                       s, prob_a, \n",
    "                                       gamma)\n",
    "                max_delta = max(max_delta, \n",
    "                                abs(v[s] - v_old))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"Converged in\", k, \"iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(\"Terminating after\", k, \"iterations.\")\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = policy_evaluation(foodtruck, policy)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The state values:\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), \n",
    "                              p=list(prob_a.values()))\n",
    "    return action\n",
    "\n",
    "def simulate_policy(policy, n_episodes):\n",
    "    np.random.seed(0)\n",
    "    foodtruck = FoodTruck()\n",
    "    rewards = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = foodtruck.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, policy)\n",
    "            state, reward, done, info = foodtruck.step(action) \n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"Expected weekly profit:\", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_policy(policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, v, s, actions, gamma):\n",
    "    prob_a = {}\n",
    "    if not env.is_terminal(s):\n",
    "        max_q = np.NINF\n",
    "        best_a = None\n",
    "        for a in actions:\n",
    "            q_sa = expected_update(env, v, s, {a: 1}, gamma)\n",
    "            if q_sa >= max_q:\n",
    "                max_q = q_sa\n",
    "                best_a = a\n",
    "        prob_a[best_a] = 1\n",
    "    else:\n",
    "        max_q = 0\n",
    "    return prob_a, max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,  eps=0.1, gamma=1):\n",
    "    np.random.seed(1)\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    policy = {s: {np.random.choice(actions): 1}\n",
    "             for s in states}\n",
    "    v = {s: 0 for s in states}\n",
    "    while True:\n",
    "        v = policy_evaluation(env, policy, v=v, \n",
    "                          eps=eps, gamma=gamma)\n",
    "        old_policy = policy\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            policy[s], _ = policy_improvement(env, v, s, \n",
    "                                    actions, gamma)\n",
    "        if old_policy == policy:\n",
    "            break\n",
    "    print(\"Optimal policy found!\")\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = policy_iteration(foodtruck)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=100, eps=0.1, gamma=1):\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    v = {s: 0 for s in states}\n",
    "    policy = {}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in states:\n",
    "            old_v = v[s]\n",
    "            policy[s], v[s] = policy_improvement(env, \n",
    "                                                 v, \n",
    "                                                 s, \n",
    "                                                 actions, \n",
    "                                                 gamma)\n",
    "            max_delta = max(max_delta, abs(v[s] - old_v))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"Converged in\", k, \"iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(\"Terminating after\", k, \"iterations.\")\n",
    "            break\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration(foodtruck)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_policy_iteration(env, max_iter=2, eps=0.1, gamma=1):\n",
    "    np.random.seed(1)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    policy = {s: {np.random.choice(actions): 1}\n",
    "             for s in states}\n",
    "    v = {s: 0 for s in states}\n",
    "    k = 0\n",
    "    while True:\n",
    "        v_old = v.copy()\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            policy[s], v[s] = policy_improvement(env, v, s, \n",
    "                                    actions, gamma)\n",
    "        v = policy_evaluation(env, policy, \n",
    "                              max_iter=max_iter, v=v, \n",
    "                              eps=eps, gamma=gamma)\n",
    "        max_delta = np.amax([abs(v[s] - v_old[s]) for s in v])\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"GPI converged in\", k, \"iterations.\")\n",
    "            print([abs(v[s] - v_old[s]) for s in v])\n",
    "            break\n",
    "            \n",
    "    print(\"Optimal policy found!\")\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = generalized_policy_iteration(foodtruck, max_iter=2, eps=0.1, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_return(returns, trajectory, gamma):\n",
    "    G = 0\n",
    "    T = len(trajectory) - 1\n",
    "    for t, sar in enumerate(reversed(trajectory)):\n",
    "        s, a, r = sar\n",
    "        G = r + gamma * G\n",
    "        first_visit = True\n",
    "        for j in range(T - t):\n",
    "            if s == trajectory[j][0]:\n",
    "                first_visit = False\n",
    "        if first_visit:\n",
    "            if s in returns:\n",
    "                returns[s].append(G)\n",
    "            else:\n",
    "                returns[s] = [G]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    sar = [state]\n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sar.append(action)\n",
    "        sar.append(reward)\n",
    "        trajectory.append(sar)\n",
    "        sar = [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, n_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    v = {}\n",
    "    for i in range(n_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns, \n",
    "                                     trajectory, \n",
    "                                     gamma)\n",
    "    for s in env.state_space:\n",
    "        if s in returns:\n",
    "            v[s] = np.round(np.mean(returns[s]), 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_est = first_visit_mc(foodtruck, policy, 1, 10000)\n",
    "v_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_true = policy_evaluation(foodtruck, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_est = first_visit_mc(foodtruck, policy, 1, 5)\n",
    "# {s: v_est[s] for s in sorted(v_est)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_est = first_visit_mc(foodtruck, policy, 1, 10)\n",
    "# {s: v_est[s] for s in sorted(v_est)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_est = first_visit_mc(foodtruck, policy, 1, 100)\n",
    "# {s: v_est[s] for s in sorted(v_est)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_est = first_visit_mc(foodtruck, policy, 1, 1000)\n",
    "# {s: v_est[s] for s in sorted(v_est)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_est = first_visit_mc(foodtruck, policy, 1, 10000)\n",
    "# {s: v_est[s] for s in sorted(v_est)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps_greedy(actions, eps, a_best):\n",
    "    prob_a = {}\n",
    "    n_a = len(actions)\n",
    "    for a in actions:\n",
    "        if a == a_best:\n",
    "            prob_a[a] = 1 - eps + eps/n_a\n",
    "        else:\n",
    "            prob_a[a] = eps/n_a\n",
    "    return prob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy(states, actions):\n",
    "    policy = {}\n",
    "    n_a = len(actions)\n",
    "    for s in states:\n",
    "        policy[s] = {a: 1/n_a for a in actions}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc(env, n_iter, eps, gamma):\n",
    "    np.random.seed(0)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    policy =  get_random_policy(states, actions)\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    Q_n = {s: {a: 0 for a in actions} for s in states}\n",
    "    for i in range(n_iter):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        G = 0\n",
    "        T = len(trajectory) - 1\n",
    "        for t, sar in enumerate(reversed(trajectory)):\n",
    "            s, a, r = sar\n",
    "            G = r + gamma * G\n",
    "            first_visit = True\n",
    "            for j in range(T - t):\n",
    "                s_j = trajectory[j][0]\n",
    "                a_j = trajectory[j][1]\n",
    "                if (s, a) == (s_j, a_j):\n",
    "                    first_visit = False\n",
    "            if first_visit:\n",
    "                Q[s][a] = Q_n[s][a] * Q[s][a] + G\n",
    "                Q_n[s][a] += 1\n",
    "                Q[s][a] /= Q_n[s][a]\n",
    "                a_best = max(Q[s].items(), \n",
    "                             key=operator.itemgetter(1))[0]\n",
    "                policy[s] = get_eps_greedy(actions, \n",
    "                                           eps, \n",
    "                                           a_best)\n",
    "    return policy, Q, Q_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q, Q_n = on_policy_first_visit_mc(foodtruck, \n",
    "                                          300000, \n",
    "                                          0.05, \n",
    "                                          1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_mc(env, n_iter, eps, gamma):\n",
    "    np.random.seed(0)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    C = {s: {a: 0 for a in actions} for s in states}\n",
    "    target_policy = {}\n",
    "    behavior_policy = get_random_policy(states, \n",
    "                                        actions)\n",
    "    for i in range(n_iter):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        trajectory = get_trajectory(env, \n",
    "                                    behavior_policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        T = len(trajectory) - 1\n",
    "        for t, sar in enumerate(reversed(trajectory)):\n",
    "            s, a, r = sar\n",
    "            G = r + gamma * G\n",
    "            C[s][a] += W\n",
    "            Q[s][a] += (W/C[s][a]) * (G - Q[s][a])\n",
    "            a_best = max(Q[s].items(), \n",
    "                         key=operator.itemgetter(1))[0]\n",
    "            target_policy[s] = a_best\n",
    "            behavior_policy[s] = get_eps_greedy(actions, \n",
    "                                                eps, \n",
    "                                                a_best)\n",
    "            if a != target_policy[s]:\n",
    "                break\n",
    "            W = W / behavior_policy[s][a]\n",
    "    target_policy = {s: target_policy[s] for s in states\n",
    "                                   if s in target_policy}\n",
    "    return target_policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = off_policy_mc(foodtruck, 300000, 0.05, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_td_prediction(env, policy, gamma, alpha, n_iter):\n",
    "    np.random.seed(0)\n",
    "    states = env.state_space\n",
    "    v = {s: 0 for s in states}\n",
    "    s = env.reset()\n",
    "    for i in range(n_iter):\n",
    "        a = choose_action(s, policy)\n",
    "        s_next, reward, done, info = env.step(a)\n",
    "        v[s] += alpha * (reward + gamma * v[s_next] - v[s])\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = s_next\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = base_policy(foodtruck.state_space)\n",
    "v = one_step_td_prediction(foodtruck, policy, 1, 0.01, 100000)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({s: np.round(v[s]) for s in v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values\n",
    "{('Mon', 0): 2515.0,\n",
    " ('Tue', 0): 1960.0,\n",
    " ('Tue', 100): 2360.0,\n",
    " ('Tue', 200): 2760.0,\n",
    " ('Tue', 300): 3205.0,\n",
    " ('Wed', 0): 1405.0,\n",
    " ('Wed', 100): 1805.0,\n",
    " ('Wed', 200): 2205.0,\n",
    " ('Wed', 300): 2650.0,\n",
    " ('Thu', 0): 850.0000000000001,\n",
    " ('Thu', 100): 1250.0,\n",
    " ('Thu', 200): 1650.0,\n",
    " ('Thu', 300): 2095.0,\n",
    " ('Fri', 0): 295.00000000000006,\n",
    " ('Fri', 100): 695.0000000000001,\n",
    " ('Fri', 200): 1095.0,\n",
    " ('Fri', 300): 1400.0,\n",
    " ('Weekend', 0): 0,\n",
    " ('Weekend', 100): 0,\n",
    " ('Weekend', 200): 0,\n",
    " ('Weekend', 300): 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, gamma, eps, alpha, n_iter):\n",
    "    np.random.seed(0)\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    policy = get_random_policy(states, actions)\n",
    "    s = env.reset()\n",
    "    a = choose_action(s, policy)\n",
    "    for i in range(n_iter):\n",
    "        if i % 100000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        s_next, reward, done, info = env.step(a)\n",
    "        a_best = max(Q[s_next].items(), \n",
    "                     key=operator.itemgetter(1))[0]\n",
    "        policy[s_next] = get_eps_greedy(actions, eps, a_best)\n",
    "        a_next = choose_action(s_next, policy)\n",
    "        Q[s][a] += alpha * (reward \n",
    "                            + gamma * Q[s_next][a_next] \n",
    "                            - Q[s][a])\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            a_best = max(Q[s].items(), \n",
    "                         key=operator.itemgetter(1))[0]\n",
    "            policy[s] = get_eps_greedy(actions, eps, a_best)\n",
    "            a = choose_action(s, policy)\n",
    "        else:\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = sarsa(foodtruck, 1, 0.1, 0.01, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[('Mon', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, eps, alpha, n_iter):\n",
    "    np.random.seed(0)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    policy = get_random_policy(states, actions)\n",
    "    s = env.reset()\n",
    "    for i in range(n_iter):\n",
    "        if i % 100000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        a_best = max(Q[s].items(), \n",
    "                     key=operator.itemgetter(1))[0]\n",
    "        policy[s] = get_eps_greedy(actions, eps, a_best)\n",
    "        a = choose_action(s, policy)\n",
    "        s_next, reward, done, info = env.step(a)\n",
    "        Q[s][a] += alpha * (reward \n",
    "                            + gamma * max(Q[s_next].values()) \n",
    "                            - Q[s][a])\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = s_next\n",
    "    policy = {s: {max(policy[s].items(), \n",
    "                 key=operator.itemgetter(1))[0]: 1}\n",
    "                 for s in states}\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = q_learning(foodtruck, 1, 0.1, 0.01, 1000000)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning(foodtruck, 1, 0.1, 0.01, 20000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{('Mon', 0): 2880.0,\n",
    " ('Tue', 0): 2250.0,\n",
    " ('Tue', 100): 2650.0,\n",
    " ('Tue', 200): 3050.0,\n",
    " ('Tue', 300): 3450.0,\n",
    " ('Wed', 0): 1620.0,\n",
    " ('Wed', 100): 2020.0,\n",
    " ('Wed', 200): 2420.0,\n",
    " ('Wed', 300): 2820.0,\n",
    " ('Thu', 0): 990.0,\n",
    " ('Thu', 100): 1390.0,\n",
    " ('Thu', 200): 1790.0,\n",
    " ('Thu', 300): 2190.0,\n",
    " ('Fri', 0): 390.00000000000006,\n",
    " ('Fri', 100): 790.0000000000001,\n",
    " ('Fri', 200): 1190.0,\n",
    " ('Fri', 300): 1400.0,\n",
    " ('Weekend', 0): 0,\n",
    " ('Weekend', 100): 0,\n",
    " ('Weekend', 200): 0,\n",
    " ('Weekend', 300): 0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
